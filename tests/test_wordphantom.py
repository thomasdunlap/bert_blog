import pytest
#from wordphantom.wordphantom import zip_concat_text
from itertools import zip_longest

all_text = [[' RoBERTa: An optimized method for pretraining self-supervised NLP systems ResearchResearch Areas: Recent PapersFundamental & AppliedRecent ProjectsResearchResearchResearch. Areas: PeopleToolsFrameworks & ToolsLibraries, Models & DatasetsBlogJoin UsJoin Us join us in the JobsAI Residency Program . We also e-e-report the results of our new research project, which is based on the widely used NLP benchmark, General Language Understanding Evaluation .', ' xplore training RoBERTa on an order of magnitude more data than BERT, for a longer amount of time . With a score of 88.5, RoberTa reached the top position on the GLUE leaderboard, matching the performance of the previous leader, XLNet-Large . Facebook’s ongoing commitment to advancing the state-of-the-art in self-supervised systems that can be developed with less reliance on time- and resource-intensive data labeling .'],
        [' BERT, RoBERTa, DistilBERT, XLNet — which one to use? | by Suleiman Khan, Ph.D. | Towards Data Science . BERT outperformed the NLP state-of-the-art on several challenging tasks . XLNet is a large bidirectional transformer that uses improved training methodology, larger data and more computational power to achieve better than BERT prediction metrics on 20 language tasks .', " DistilBERT learns a distilled (approximate) version of Google's BERT, retaining 97% performance but using only half the number of parameters . RoBERTa uses a technique called distillation, which approximates the Google’s BERT by a smaller one . The idea is that once a large neural network has been trained, its full output distributions can be approximated using a smaller network . This is in some sense similar to posterior approximation in Bayesian Statistics ."],
        [' Google’s BERT and recent transformer-based methods have taken the NLP landscape by a storm, outperforming the state-of-the-art on several tasks . Lately, varying improvements over BERT have been shown — and here I will contrast the main similarities and differences so you can choose which one to use in your research or application . BERT is a bi-directional transformer for pre-training over a lot of unlabeled textual data to learn a language representation that can be used to fine-tune for specific machine learning tasks . XLNet and RoBERTa improve on the performance while DistilBERT improves on the inference speed .', ' Facebook’s RoBERTa outperforms both BERT and XLNet on GLUE benchmark results . DistilBERT learns a distilled (approximate) version of BERT, retaining 95% performance but using only half the number of parameters . RoBERta uses 160 GB of text for pre-training, including 16GB of Books Corpus and English Wikipedia used in BERT . The additional data included CommonCrawl News dataset (63 million articles, 76 GB) and Web text corpus (38 GB)', ' How to Acquire the Most Wanted Data Science Skills Learn to build an end to end data science project How to Get Into Data Science Without a Degree How to Explain Key Machine Learning Algorithms at an Interview  Top 5 Free Machine Learning and Deep Learning eBooks Everyone should read . How to Know if a Neural Network is Right for Your Machi... Cartoon: Thanksgiving and Turkey Data Science . Better data apps with Streamlit’s new layout options .'],
        [' BERT outperforms previous methods because it is the first unsupervised, deeply bidirectional system for pre-training NLP . RoBERTa (Robustly Optimized BERT approach) is introduced and performance is either matching or exceeding original BERT . BERT was significantly undertrained, and can match or exceed the performance of every model published after it . The new approach uses BookCorpus (16G), CC-NEWS (76G), OpenWebText (38G) and Stories (31G) data .', ' BERT-BASE (Devlin et al., 2018) is trained via 1M steps with a batch size of 256 sequences . While Lie et al. trained 125k steps with 2k sequences achieve a better result . RoBERTa: A Robustly Optimized BERT Pretraining Approach. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. 2018Y. Liu, M. Ott, N. Goyal, J. Du, D. Joshi, M., Du, Du, Chen, O. Levy, L. Zettlemoyer, and V. Stoyanov .']]

def test_zip_concat_text():
    print(f"{list(zip_longest(*all_text))}")
    assert True
ret = '\n'.join(' '.join(tup) for tup in zip_longest(*all_text, fillvalue=' '))
print(f"{ret}")

